{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1209cb90",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Project: Action Recognition using CNN + LSTM (PyTorch)\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from pathlib import Path\n",
        "# Project root: works when run from repo root or from notebooks/\n",
        "try:\n",
        "    PROJECT_ROOT = Path(__file__).resolve().parent.parent\n",
        "except NameError:\n",
        "    PROJECT_ROOT = Path.cwd() if (Path.cwd() / \"src\" / \"vllmd\").exists() else Path.cwd().parent\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n",
        "# Make the package importable as \"vllmd\" (package lives under src/vllmd)\n",
        "_src = PROJECT_ROOT / \"src\"\n",
        "if _src.exists() and str(_src) not in sys.path:\n",
        "    sys.path.insert(0, str(_src))\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import torch  # pyright: ignore[reportMissingImports]\n",
        "import torch.nn as nn  # pyright: ignore[reportMissingImports]\n",
        "from torch.utils.data import DataLoader  # pyright: ignore[reportMissingImports]\n",
        "from torch.optim import Adam  # pyright: ignore[reportMissingImports]\n",
        "import torchvision.models as models  # pyright: ignore[reportMissingImports]\n",
        "from vllmd.video_processing import VideoDataProcessor, VideoDataset, ActionRecognitionPipeline, describe_frames_after_predict_each_frame\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3c439c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths relative to project root (use data/ for datasets)\n",
        "DIRECTORY = PROJECT_ROOT / \"data\" / \"abnormal\"\n",
        "NORMAL_DIRECTORY = PROJECT_ROOT / \"data\" / \"normal\"\n",
        "PATH_VIDEO_TEST = PROJECT_ROOT / \"data\" / \"test\" / \"19.mp4\"\n",
        "RULE_PATH = PROJECT_ROOT / \"data\" / \"rules\" / \"entity_rules.txt\"\n",
        "PATH_MODEL = PROJECT_ROOT / \"models\" / \"best_model.pt\"\n",
        "\n",
        "DIRECTORY = str(DIRECTORY)\n",
        "NORMAL_DIRECTORY = str(NORMAL_DIRECTORY)\n",
        "PATH_VIDEO_TEST = str(PATH_VIDEO_TEST)\n",
        "RULE_PATH = str(RULE_PATH)\n",
        "PATH_MODEL = str(PATH_MODEL)\n",
        "\n",
        "processor = VideoDataProcessor(DIRECTORY, NORMAL_DIRECTORY, frame_size=64, num_frames=40)\n",
        "selected_classes = processor.selected_classes\n",
        "Frame_Size = processor.frame_size\n",
        "print(selected_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8642e2c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df = processor.build_dataframe()\n",
        "print(train_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bf2a155",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Saving original frame without any resizing\n",
        "from PIL import Image\n",
        "\n",
        "for class_index, cls in enumerate(selected_classes):\n",
        "    if cls == \"Normal Videos\":\n",
        "        class_path = os.path.join(NORMAL_DIRECTORY, cls)\n",
        "        videos = os.listdir(class_path)[:30]\n",
        "    else:\n",
        "        class_path = os.path.join(DIRECTORY, cls)\n",
        "        videos = os.listdir(class_path)[:30]\n",
        "    for video in videos:\n",
        "        video_path = os.path.join(class_path, video)\n",
        "        frames = processor.extract_original_frames(video_path, num_frames=40)\n",
        "        a = frames.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28cf89ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = processor.load_training_arrays(videos_per_class=30, test_size=0.2, random_state=42)\n",
        "print(\"Training:\", len(X_train), \"Testing:\", len(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3270ce5b",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = VideoDataset(X_train, y_train)\n",
        "val_dataset = VideoDataset(X_test, y_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b909353d",
      "metadata": {},
      "outputs": [],
      "source": [
        "num_classes = len(selected_classes)\n",
        "pipeline = ActionRecognitionPipeline(num_classes, device, frame_size=Frame_Size, num_frames=40)\n",
        "model = pipeline.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60782ba0",
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline.train(train_loader, val_loader, epochs=1, save_path=str(PATH_MODEL))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5fc80f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline.load(str(PATH_MODEL))\n",
        "# pipeline.show_predictions(X_test, y_test, selected_classes, num_samples=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbd4e814",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run prediction on test video to get frame_indices, frames, frames_original\n",
        "frame_indices, preds, probs, frames, frames_original = pipeline.predict_each_frame(\n",
        "    str(PATH_VIDEO_TEST), processor, selected_classes\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae946008",
      "metadata": {},
      "outputs": [],
      "source": [
        "# External API image description (5 LLM models: OpenAI, Anthropic, Google)\n",
        "from vllmd.llm import ExternalLLMImageDescriber, MODEL_NAMES\n",
        "\n",
        "# Optional: set API keys here or via env (OPENAI_API_KEY, ANTHROPIC_API_KEY, GOOGLE_API_KEY)\n",
        "describer = ExternalLLMImageDescriber(\n",
        "    default_model=\"gemini-3-flash-preview\",  # advanced Gemini; or \"gemini-2.0-flash\", \"gpt-4o-mini\", etc.\n",
        ")\n",
        "\n",
        "# Describe a few predicted frames with the external API (uses frames from cell above)\n",
        "num_frames_to_describe = 4\n",
        "show_indices = np.linspace(0, len(frame_indices) - 1, min(num_frames_to_describe, len(frame_indices)), dtype=int)\n",
        "frames_to_describe = [frames_original[i] if frames_original is not None else frames[i] for i in show_indices]\n",
        "\n",
        "external_descriptions = []\n",
        "for i, frame in enumerate(frames_to_describe):\n",
        "    desc = describer.describe_frame(frame, prompt=\"Describe this video frame in 2-3 short sentences.\")\n",
        "    external_descriptions.append(desc)\n",
        "    print(f\"Frame {frame_indices[show_indices[i]]} [{describer.default_model}]: {desc}\")\n",
        "    #print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48954da5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display each described frame and its description in a two-column table\n",
        "from vllmd.utils import display_frames_with_descriptions\n",
        "\n",
        "display_frames_with_descriptions(\n",
        "    frames_to_describe,\n",
        "    external_descriptions,\n",
        "    frame_indices=[frame_indices[show_indices[i]] for i in range(len(frames_to_describe))],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e168ac72",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom entity extraction on frame descriptions (regex + keywords)\n",
        "from vllmd.utils import load_rules_from_file\n",
        "custom = load_rules_from_file(RULE_PATH, merge_overlaps=True)\n",
        "\n",
        "custom_entities_per_frame = custom.extract_batch(external_descriptions, merge_duplicates=True)\n",
        "\n",
        "print(\"Custom entities per frame description (ACTION / OBJECT / SCENE):\")\n",
        "for i, (desc, entities) in enumerate(zip(external_descriptions, custom_entities_per_frame)):\n",
        "    print(f\"\\nFrame {i}: {desc}\")\n",
        "    if entities:\n",
        "        print(f\"  -> {[str(e) for e in entities]}\")\n",
        "    else:\n",
        "        print(\"  -> (none)\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "thesis-torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
